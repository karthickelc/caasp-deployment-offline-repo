In this hands on lab exercise, we will look at how to setup your offline environment for application delivery using Offline repo method. 

Lab 1 : Setup RMT server and Mirror repositories 
Lab 2 : Create offline Docker registry
Lab 3 : Setup reverse proxy and virtual hosts
Lab 4 : Mirror CaasP and CAP images locally
Lab 5 : Client side changes to access local registry

For this lab you will have to create following Vms ,

    1. DNS sever
    2. RMT server
    3. Management server
    4. Master node
    5. Worker01

Lab 1: Setup RMT server and Mirror repositories
 
1. On your lab environment, open the Virtual Machine manager and connect to rmt virtual machine console .  When the server has booted and presents a login screen, close the console window 

2. When the login screen is displayed, use the following credentials:
User: tux
Password: linux

3. RMT server is pre-installed in RMT VM with base SLES 15Sp1 . We will have to install the rmt-server pattern and then configure it . RMT server will need internet connectivity in order to download all the system patches and registry images. 

# su
# zypper lr | grep Server_Applications_Module
SUSEConnect --product sle-module-server-applications/15.1/x86_64
# zypper in rmt-server 
# yast2 rmt
This will take you through rmt-server wizard. You will have to update your respective SCC organization credentials. Also provide values for the DB username and password along with CA password. While creating CA certification provide aliases for charts.example.com and registry.example.com as well . Complete and close the wizard.  

Check the status of the rmt server by issuing below command,

# systemctl status rmt-server.service 

Should return an output similar to below output with Active: active ,  rmt systemd[1]: Started

● rmt-server.service - RMT API server
   Loaded: loaded (/usr/lib/systemd/system/rmt-server.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2020-04-09 20:40:00 IST; 16min ago
 Main PID: 4247 (rails)
    Tasks: 12 (limit: 4915)
   CGroup: /system.slice/rmt-server.service
           └─4247 puma 3.10.0 (tcp://127.0.0.1:4224) [rmt]

Apr 09 20:40:00 rmt systemd[1]: Started RMT API server.

4. Sync anand List all the available products by issuing below command 

# rmt-cli products list --all

This will list the available product repositories to be mirrored offline similar to below output ,

+------+----------------------+---------+--------+--------------+---------------
| ID   | Product              | Version | Arch   | Mirror?      | Last mirrored
+------+----------------------+---------+--------+--------------+---------------
[...]
| 1743 | SUSE Package Hub     | 15      | x86_64 | Don't Mirror |
|      | PackageHub/15/x86_64 |         |        |              |
[...]
The mirror ID can be used to enable or disable a repository . Below repositories must be enabled for our CaaSP and CAP deployment 

| 1772 | Basesystem Module                                  			    | 15 SP1  | x86_64  | 
| 1790 | Containers Module                                   			    | 15 SP1  | x86_64  | 
| 1867 | Python 2 Module                                      			    | 15 SP1  | x86_64  | 
| 1780 | Server Applications Module                			    | 15 SP1  | x86_64  | 
| 1863 | SUSE CaaS Platform                             			    | 4.0         | x86_64  | 
| 1809 | SUSE Cloud Application Platform Tools Module     	    | 15 SP1  | x86_64  |        
| 1763 | SUSE Linux Enterprise Server                     		    | 15 SP1  | x86_64  | 
| 1871 | SUSE Package Hub                           			                | 15 SP1  | x86_64  | 




5.Enable SUSE Cloud Application Platform Tools Module repository with 

# rmt-cli products enable 1809 
should enable the mandatory repositories related to  SUSE Cloud Application Platform Tools Module . The out put should be similar to ,

Found product by target 1809: SUSE Cloud Application Platform Tools Module 15 SP1 x86_64.
Enabling SUSE Cloud Application Platform Tools Module 15 SP1 x86_64:
  SUSE Cloud Application Platform Tools Module 15 SP1 x86_64:
    Enabled repository SLE-Module-CAP-Tools15-SP1-Pool.
    Enabled repository SLE-Module-CAP-Tools15-SP1-Updates.

6. Now mirror the repositories enabled in previous step ,

#  rmt-cli mirror 

The mirror should work without any error .Similarly enable rest of the product repositories mentioned in step 4 ie , rmt-cli products enable 1763 1772 1790 1867 1780 1863 1871 1776 1809 and mirror the same. Mirrored repositories are available under /usr/share/rmt/public/repo/SUSE/Products/ 







































Lab 2: Create offline Docker registry 

1. Install the docker-distribution-registry package on rmt server vm . This package is available in SUSE Packagehub module . 

# zypper refresh 

2. Now install Docker distribution registry 

# zypper install docker docker-distribution-registry

edit docker config file to match below configuration
rmt:~ # cat /etc/registry/config.yml 
version: 0.1
log:
  level: info
storage:
  filesystem:
    rootdirectory: /var/lib/registry
http:
  addr: 0.0.0.0:5000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /etc/rmt/ssl/rmt-server.crt
    key: /etc/rmt/ssl/rmt-server.key
health:
  storagedriver:
    enabled: true
    interval: 10s
threshold: 3
*********************************************************************
Note : If your registry is insecure disable registry security verification by editing
/etc/docker/daemon.json to ,
 
{
  "insecure-registries" : ["rmt.example.com:5000"]
}
***********************************************************************************






create the directory registry under /var/lib
# mkdir /var/lib/registry

3. Make sure CA certificate is available to SUSE CaaS system wide 

# cp /etc/rmt/ssl/rmt-ca.crt /etc/pki/trust/anchors/
# update-ca-certificates


4. Enable and start Docker registry on boot
# systemctl enable --now registry

5. Enable and start Docker service on boot  
# systemctl enable --now docker

Now that the offline registry is setup let us test if the registry is working using following steps ,

7. Test image pull 
# docker pull registry.suse.com/suse/sle15:15.1

8. Tag the image 
# docker tag registry.suse.com/suse/sle15:15.1 rmt.example.com:5000/suse/sle15:15.1

9. Push the image to Local repository
# docker push rmt.example.com:5000/suse/sle15:15.1 

Note : above command should result with status pushed . 




















Lab 3:Setup reverse proxy and virtual host 

Create reverse proxy and virtual host configuration file for registry  

 Login to rmt server and create /etc/nginx/vhosts.d/registry-server-https.conf . Replace mymirror.local with the hostname of your mirror server for which you created the SSL certificates. 

# vim  /etc/nginx/vhosts.d/registry-server-https.conf

*********************************************************************

upstream docker-registry {
    server 127.0.0.1:5000;
}

map $upstream_http_docker_distribution_api_version $docker_distribution_api_version {
  '' 'registry/2.0';
}

server {
    listen 443   ssl;
    server_name  registry.example.com;

    access_log  /var/log/nginx/registry_https_access.log;
    error_log   /var/log/nginx/registry_https_error.log;
    root        /usr/share/rmt/public;

    ssl_certificate     /etc/rmt/ssl/rmt-server.crt;
    ssl_certificate_key /etc/rmt/ssl/rmt-server.key;
    ssl_protocols       TLSv1.2 TLSv1.3;

    # disable any limits to avoid HTTP 413 for large image uploads
    client_max_body_size 0;

    location /v2/ {
      # Do not allow connections from docker 1.5 and earlier
      # docker pre-1.6.0 did not properly set the user agent on ping, catch "Go *" user agents
      if ($http_user_agent ~ "^(docker\/1\.(3|4|5(?!\.[0-9]-dev))|Go ).*$" ) {
        return 404;
      }

      ## If $docker_distribution_api_version is empty, the header is not added.
      ## See the map directive above where this variable is defined.
      add_header 'Docker-Distribution-Api-Version' $docker_distribution_api_version always;

      proxy_pass                          http://docker-registry;
      proxy_set_header  Host              $http_host;   # required for docker client's sake
      proxy_set_header  X-Real-IP         $remote_addr; # pass on real client's IP
      proxy_set_header  X-Forwarded-For   $proxy_add_x_forwarded_for;
      proxy_set_header  X-Forwarded-Proto $scheme;
      proxy_read_timeout                  900;
    }
}
Save the file and restart nginx 
# systemctl restart nginx

Create a link of registry folder at document root’s folder 
# ln -s /var/lib/registry/docker/registry/v2 /usr/share/rmt/public/

check the file permission 
# ls -la /usr/share/rmt/public/

total 12
drwxr-xr-x  3 _rmt nginx 4096 Apr 27 02:44 .
drwxr-xr-x 12 _rmt nginx 4096 Apr 26 15:41 ..
lrwxrwxrwx  1 _rmt nginx   24 Apr  3 18:48 repo -> /var/lib/rmt/public/repo
lrwxrwxrwx  1 _rmt nginx   24 Apr  3 18:48 suma -> /var/lib/rmt/public/suma
drwxr-xr-x  2 _rmt nginx 4096 Apr 26 15:41 tools
lrwxrwxrwx  1 root root    36 Apr 27 02:44 v2 -> /var/lib/registry/docker/registry/v2

now softlink is created for registry under /usr/share/rmt/public . You will have to change the ownership of this link from root:root to _rmt:nginx

# chown -R _rmt:nginx /usr/share/rmt/public/v2

rmt:~ # 
total 12
drwxr-xr-x  3 _rmt nginx 4096 Apr 27 02:44 .
drwxr-xr-x 12 _rmt nginx 4096 Apr 26 15:41 ..
lrwxrwxrwx  1 _rmt nginx   24 Apr  3 18:48 repo -> /var/lib/rmt/public/repo
lrwxrwxrwx  1 _rmt nginx   24 Apr  3 18:48 suma -> /var/lib/rmt/public/suma
drwxr-xr-x  2 _rmt nginx 4096 Apr 26 15:41 tools
lrwxrwxrwx  1 _rmt nginx   36 Apr 27 02:44 v2 -> /var/lib/registry/docker/registry/v2

You can confirm if the ownership is changed . Restart the nginx service 
# systemctl restart nginx







On your browser go to below URL and verify if your registry catalog is listed, 
https://registry.example.com:5000/v2/_catalog
2.Create a virtual host configuration file for Helm charts repository 
# vim /etc/nginx/vhosts.d/charts-server-https.conf
server {
  listen 443   ssl;
  server_name  charts.example.com;

  access_log  /var/log/nginx/charts_https_access.log;
  error_log   /var/log/nginx/charts_https_error.log;
  root        /srv/www/;

  ssl_certificate     /etc/rmt/ssl/rmt-server.crt;
  ssl_certificate_key /etc/rmt/ssl/rmt-server.key;
  ssl_protocols       TLSv1.2 TLSv1.3;

  location /charts {
    autoindex on;
  }
}
save the file . Then create directory charts under /srv/www

# mkdir /srv/www/charts/
Change the file permission and ownership
2. chown -R nginx:nginx /srv/www/charts
3. chmod -R 555 /srv/www/charts/
4. touch /srv/www/charts/testindex.html

Restart nginx for the changes to take effect.
# systemctl restart nginx
On your browser go to below URL and verify if you can access below URL,
https://charts.example.com/charts/

3. Mirror Helm charts offline 

1. Install Helm and Helm-mirror . These tools are needed to mirror content from online repositories to Offline registry 
# zypper in helm helm-mirror 

2. Initialize helm and tiller
# helm init --tiller-image registry.suse.com/caasp/v4/helm-tiller:2.16.1 --service-account tiller
Should initialize helm and tiller with errors . You can ignore the error for now.Make sure the tiller version is same as helm version .

4. Mirror the helm charts to /srv/www/charts/
# helm-mirror --new-root-url https://charts.example.com/charts https://kubernetes-charts.suse.com /srv/www/charts/
Note: --new-root-url will append the local chart repositories URL on the index.yaml file 
refresh your https://charts.example.com/charts/ URL in browser you should see helm charts with index.yaml file mirrored offline . Now offline helm chart repo is configured .



Lab 4:Mirror CaaSP and CAP container images to offline registry 

We will need images from registry.suse.com to be mirrored offline for our local repo to work .Official CaaSP images are listed here ,
https://documentation.suse.com/external-tree/en-us/suse-caasp/4/skuba-cluster-images.txt 

Login to your rmt server as root and create these mirror scripts . We will use these commands to mirror and copy the content into offline repository

1. Check if the docker service is up and running
# systemctl status docker.service 
 Should return ,
docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) 
2.   Create the mirror scripts under location of your choice as caasmirror.sh
thirdpartymirror.sh and  capmirror.sh
3.  With editor of your choice create caasmirror.sh with below content . 
# vim caasmirror.sh 
*******************************************************************************
#!/bin/bash
# "skuba cluster images" to get the latest bootstrap images
MIRROR="registry.example.com:5000" #(This should match the FQDN of your offline registry)
REMOTE="registry.suse.com/caasp/v4"
IMAGES="
hyperkube:v1.16.2
hyperkube:v1.15.2
etcd:3.3.15
etcd:3.3.11
coredns:1.6.2
coredns:1.3.1
pause:3.1
skuba-tooling:0.1.0
cilium-init:1.5.3
cilium-operator:1.5.3
cilium:1.5.3
caasp-dex:2.16.0
gangway:3.1.0-rev4
kured:1.2.0-rev4
"
for IMAGE in ${IMAGES}; do
        echo $IMAGE
        docker pull $REMOTE/$IMAGE
        docker tag $REMOTE/$IMAGE $MIRROR/$REMOTE/$IMAGE
        docker push $MIRROR/$REMOTE/$IMAGE
done

***********************************************************************************
6. Now that the mirror is setup execute the caasmirror.sh script
# sh caaspmirror.sh 
Allow the script to complete . Once completed this script will mirror the content of registry.suse.com/caasp/v4 to offline registry  registry.example.com:5000 . 










7.  With editor of your choice create capmirror.sh with following content
Note : Currently the SUSE Cloud Application Platform version is 1.5.2 . These mirror is valid for CAP 1.5.2 
# vim capmirror.sh 
***********************************************************************************
#!/bin/bash
MIRROR="registry.example.com:5000" 
REMOTE="registry.suse.com/cap"
IMAGES="
recipe-downloader:0.30.0
recipe-executor:0.31.0
recipe-uploader:0.28.0
scf-adapter:c306b00f317984c17c7a16e7895e664152754725
scf-api-group:240bba58ea8c6816e874d0df72bec5e6a7288bd8
scf-autoscaler-actors:76a2de51e3236e9bfbfb4037c839a6f966d4ec51
scf-autoscaler-api:6dda56e79f76a07d755acdf5f0787a5f5bb5e0de
scf-autoscaler-metrics:bed854fffa70100254372ec6e860e13ea9131934
scf-autoscaler-postgres:87654c0891279508582d31167ba3552739507b5a
scf-bits:98c80aec4ca157e70004a3e22b08a53fe529e9b8
scf-blobstore:e9843533b9b50c8732c26a1f019c32c0e5d1d026
scf-cc-clock:5979bc5c9f23499889b870eee3a473dadf32c458
scf-cc-uploader:0da2a1e77e892ad37f09c512e34f3f08ec28379d
scf-cc-worker:311734ce2f5a8b3c7d3661233f4ca8f34ec2e399
scf-cf-usb-group:43fd573f368c9298867830eae61503182198005d
scf-configgin-helper:f9d9c2c3d04ad90d1609b7b31b5548979c54b17f
scf-configure-eirini:16243b323f8a90221d32c9fd1e40dea521dc2487
scf-credhub-user:22b8e050b92b3b9f044166460e83a358d3bb34af
scf-diego-api:89981b0a374f50ff4c97558f2c4ea2abf325da8d
scf-diego-brain:d5c5ea22e4535230579c6b25d7b709fa60dc1e4b
scf-diego-cell:735bec6bc52438a4b73b670ad4a4c2959b82ae69
scf-diego-ssh:b5d6f4e69a3d51a6f17618caba4e3352cd6fca7c
scf-doppler:da14867a9ba02904e7986db10b6fb70ad1b6eab5
scf-eirini-persi:8158bc02c4ed40d622fc33ae506ca1ce4ed8ca3f
scf-eirini-ssh:6ca2c078eeb69357b9d5dc901577ebf96e7c994d
scf-eirini:155759633d709645f18b2e3cdced9183d0f0169f
scf-locket:f075cf11061939b5582b2f6cd41defd902f8f8da
scf-log-api:964528e7a74b1cc28bd9b934d30ef28f0fcc1ced
scf-loggregator-agent:98a13dd53091f3cfc60126204d00aa5033ea7e1f
scf-mysql-proxy:3e3d60899c81a9736cb4f36db2d73a8c5a8cc4b7
scf-mysql:d7e00908f08c51769925d5fbaee039ea9282705f
scf-nats:ea3de18b750ca894e7062f7b060a22ee4f019c0e
scf-nfs-broker:fdbf98742036fd626f96fa27539b90bf43845fe3
scf-post-deployment-setup:4d437fbe87dae722cfc235f75cc4f7774b335330
scf-router:4218c48f670db8b16ae19690d1dcf2c44c4312ba
scf-routing-api:099258688e494ba27d13333f98e557bcf90cab5e
scf-secret-generation:9d16624f9a6e8131119e3efbf6ff555f14822ddf
scf-syslog-scheduler:d2aee32c52df5a9a93f34f5e6f729f31b03be7da
scf-tcp-router:821f7f863c989a53ee00d65936360be2c2c05dc7
scf-uaa:c2c5e587774a14e04c429288d10b9a84a42d48c4

cf-usb-sidecar-mysql:1.0.1
cf-usb-sidecar-mysql-setup:1.0.1

uaa-configgin-helper:7ef898a83f98f20b3340e88760e99e30c60081c0
uaa-mysql-proxy:797ce1924c85379bf1c83e830955d528597cc832
uaa-mysql:903a2ac9b66484f32137b9029b63ce845695635f
uaa-post-deployment-setup:18a5bef2ffe2f8dc43b47f0fc1c34266c49623cf
uaa-secret-generation:0653dd4863841f90cb585d36907afdfb7c1369bf
uaa-uaa:9de4f84fcaf0672b7488cc0c77342ed955c87e2e
stratos-metrics-cf-exporter:1.1.2-85daaa2-cap
stratos-metrics-firehose-exporter:1.1.2-85daaa2-cap
stratos-metrics-firehose-init:1.1.2-85daaa2-cap
stratos-metrics-nginx:1.1.2-85daaa2-cap
stratos-metrics-configmap-reload:1.1.2-85daaa2-cap
stratos-metrics-init-chown-data:1.1.2-85daaa2-cap
stratos-metrics-kube-state-metrics:1.1.2-85daaa2-cap
stratos-metrics-node-exporter:1.1.2-85daaa2-cap
stratos-metrics-prometheus:1.1.2-85daaa2-cap
minibroker:1b8db9db6bd9b20448599d81dee0ebcd896c1c43
stratos-chartsync:3.0.0-8fbc06b06-cap
stratos-config-init:3.0.0-8fbc06b06-cap
stratos-console:3.0.0-8fbc06b06-cap
stratos-fdbdoclayer:3.0.0-8fbc06b06-cap
stratos-fdbserver:3.0.0-8fbc06b06-cap
stratos-jetstream:3.0.0-8fbc06b06-cap
stratos-mariadb:3.0.0-8fbc06b06-cap
"
for IMAGE in ${IMAGES}; do
        echo $IMAGE
        docker pull $REMOTE/$IMAGE
        docker tag $REMOTE/$IMAGE $MIRROR/$IMAGE
        docker push $MIRROR/$IMAGE
    done
***********************************************************************************

8. With editor of your choice create nonsuserepo.sh with below content,

# vim nonsuserepo.sh 
*********************************************************************
#!/bin/bash
MIRROR="registry.example.com:5000" 
IMAGES="
quay.io/external_storage/nfs-client-provisioner:latest
splatform/minibroker:latest
gcr.io/google-samples/node-hello:1.0
gcr.io/google_containers/metrics-server-amd64:v0.3.6
gcr.io/google_containers/kubernetes-dashboard-amd64:v1.10.0
docker.io/mhausenblas/simpleservice:0.5.0
docker.io/nginx:1.12.0
docker.io/nginx:1.7.9
docker.io/nginx:1.9.0
docker.io/busybox
gcr.io/google_containers/hpa-example
gcr.io/google_containers/busybox:1.24
gcr.io/kubernetes-helm/tiller:v2.16.1
quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
docker.io/metallb/controller:v0.8.1
docker.io/metallb/speaker:v0.8.1
docker.io/hashicorp/http-echo
docker.io/opensuse/leap 
docker.io/mysql:5.6
registry.suse.com/sles12/nginx-ingress-controller:0.15.0
registry.suse.com/sles12/default-http-backend:0.15.0
"
for IMAGE in ${IMAGES}; do
        echo $IMAGE
        docker pull $IMAGE
        docker tag $IMAGE $MIRROR/$IMAGE
        docker push $MIRROR/$IMAGE
done
*********************************************************************
10 . Once the scripts caaspmirror.sh ,capmirror.sh and nonsuserepo.sh are executed without any error , the registries needed to setup offline CaaSP and CAP environment  are mirrored to your local registry . You can verify the offline container images by issuing 
#  docker images 
or refresh https://registry.example.com:5000/v2/_catalog on your browser .










Lab 5 : Client side changes to access local registry 
We have our RMT server and registry server ready now and we can use them to install SLES and then SUSE CaaSP and CAP. We need to perform below tasks on all SUSE CaaSP cluster machines(master, worker and management nodes) so that we can deploy SUSE CaaSP and CAP offline and deliver cloud native applications . 
First make sure the clients are updated with rmt server’s CA certificate 
# scp root@rmt.example.com:/etc/rmt/ssl/rmt-ca.crt /etc/pki/trust/anchors/
# update-ca-certificates
Configure /etc/containers/registries.conf to setup the mirroring from registry.suse.com to the internal mirror. This needs to be done on all cluster nodes. Make sure to adjust all the correct domain name for your local registry:
# vim /etc/containers/registries.conf
[[registry]]
prefix = "registry.suse.com"
location = "registry.example.com:5000/registry.suse.com"
## insecure = true ## Optional: if the registry is not secure this can be set
prefix = "docker.io/library"
location = "registry.example.com:5000/docker.io"
[[registry]]
prefix = "quay.io"
location = "registry.example.com:5000/quay.io"
[[registry]]
prefix = "k8s.gcr.io"
location = "registry.example.com:5000/k8s.gcr.io"
[[registry]]
prefix = "gcr.io"
location = "registry.example.com:5000/gcr.io"
Check if docker can pull images from local registry ,
# docker pull registry.example.com:5000/registry.suse.com/caasp/v4/pause:3.1
Once everything setup you can follow below link to setup SUSE CaaSP cluster.
https://documentation.suse.com/suse-caasp/4.1/single-html/caasp-deployment/index.html#deployment_bare_metal
Once SUSE CaaSP cluster is up and running follow below link to setup SUSE CAP.
https://documentation.suse.com/suse-cap/1.5/single-html/cap-guides/index.html#cha-cap-depl-caasp

